# LLM provider configuration
defaultProvider: gemini
providers:
  openai:
    baseUrl: https://api.openai.com
    apiKey: "YOUR_API_KEY"
    defaultModel: gpt-4o
    rateLimiting:
      requestsPerMinute: 60
      maxConcurrent: 5
      retryDelay: 1000
  anthropic:
    baseUrl: https://api.anthropic.com
    apiKey: "YOUR_API_KEY"
    defaultModel: claude-3-sonnet-20240229
    rateLimiting:
      requestsPerMinute: 60
      maxConcurrent: 5
      retryDelay: 1000
  gemini:
    baseUrl: https://generativelanguage.googleapis.com
    apiKey: "YOUR_API_KEY"
    defaultModel: gemini-1.5-flash
    rateLimiting:
      requestsPerMinute: 60
      maxConcurrent: 5
      retryDelay: 1000
  ollama:
    protocol: http
    baseUrl: localhost
    port: 11434
    defaultModel: llama2
    rateLimiting:
      requestsPerMinute: 300
      maxConcurrent: 10
      retryDelay: 500
requestDefaults:
  temperature: 0.7
  maxTokens: 1000
  topP: 1.0